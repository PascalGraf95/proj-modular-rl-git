ActorTrainingFreq: 1
BatchSize: 64
ClipGrad: 1.0
Episodes: 1000000
ExplorationParameters:
  BatchSize: 128
  Epsilon: 1.0
  EpsilonDecay: 0.999
  EpsilonMin: 0.0
  FeatureSpaceSize: 288
  ForwardModelLearningRate: 0.001
  InverseModelLearningRate: 0.001
  ScalingFactor: 0.01
  StepDown: 200
Gamma: 0.99
LearningRateActor: 0.0005
LearningRateCritic: 0.0005
LogAlpha: 0.1
NSteps: 4
NetworkParameters:
- Filters: 32
  Input:
  - &id001 !!python/tuple
    - 13
  KernelInitializer: RandomUniform
  NetworkType: Actor
  Output:
  - 4
  - 4
  OutputActivation:
  - null
  - null
  Units: 64
  VectorNetworkArchitecture: Dense
  VisualNetworkArchitecture: CNNBatchnorm
- Filters: 32
  Input: &id002
  - *id001
  - 4
  KernelInitializer: RandomUniform
  NetworkType: Critic1
  Output: &id003
  - 1
  OutputActivation: &id004
  - null
  TargetNetwork: true
  Units: 64
  Vec2Img: false
  VectorNetworkArchitecture: Dense
  VisualNetworkArchitecture: CNNBatchnorm
- Filters: 32
  Input: *id002
  KernelInitializer: RandomUniform
  NetworkType: Critic2
  Output: *id003
  OutputActivation: *id004
  TargetNetwork: true
  Units: 64
  Vec2Img: false
  VectorNetworkArchitecture: Dense
  VisualNetworkArchitecture: CNNBatchnorm
PrioritizedReplay: false
ReplayCapacity: 100000
ReplayMinSize: 2000
SyncMode: soft_sync
SyncSteps: 50
Tau: 0.01
TrainingDescription: SAC_VectorPushingCurriculum_NormalReward_PlusRotation
TrainingID: SAC_VectorPushingCurriculum_NormalReward_PlusRotation
TrainingInterval: 1
ActionShape: 4
ActionType: CONTINUOUS
AgentNumber: 1
BehaviorName: My Behavior?team=0
ObservationShapes:
- !!python/tuple
  - 13
ActionAltering: false
IntrinsicReward: false
Name: None
ParameterSpace: {}
